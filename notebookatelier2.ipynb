{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13946328,"sourceType":"datasetVersion","datasetId":8888798}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport struct\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor  # Import fixé\nfrom torchvision.models import vgg16, alexnet\nfrom sklearn.metrics import accuracy_score, f1_score\nimport time\nimport matplotlib.pyplot as plt\n\n# Chemin dataset\ndata_path = '/kaggle/input/dataset-atelier2'\n\ndef load_mnist(path, kind='train'):\n    labels_path = os.path.join(path, f'{kind}-labels-idx1-ubyte')\n    images_path = os.path.join(path, f'{kind}-images-idx3-ubyte')\n    \n    with open(labels_path, 'rb') as lbpath:\n        magic, n = struct.unpack('>II', lbpath.read(8))\n        labels = np.fromfile(lbpath, dtype=np.uint8)\n    \n    with open(images_path, 'rb') as imgpath:\n        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))\n        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 28, 28)  # Fix: [N, H, W] sans channel dim\n    \n    return images, labels\n\ntrain_images, train_labels = load_mnist(data_path, 'train')\ntest_images, test_labels = load_mnist(data_path, 't10k')\n\nclass MNISTDataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images.astype(np.float32) / 255.0  # [N, H, W]\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img = self.images[idx]  # [H, W]\n        if self.transform:\n            img = self.transform(img)  # ToTensor ajoute [C=1, H, W]\n        return img, self.labels[idx]\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\ntrain_set = MNISTDataset(train_images, train_labels, transform)\ntest_set = MNISTDataset(test_images, test_labels, transform)\n\ntrain_loader = DataLoader(train_set, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:18:29.998997Z","iopub.execute_input":"2025-12-01T18:18:29.999274Z","iopub.status.idle":"2025-12-01T18:18:30.142618Z","shell.execute_reply.started":"2025-12-01T18:18:29.999243Z","shell.execute_reply":"2025-12-01T18:18:30.141921Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)  # Régularisation\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n        x = self.relu(self.conv2(x))\n        x = self.pool(x)\n        x = x.view(-1, 64 * 7 * 7)\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\ncnn_model = SimpleCNN().to(device)\noptimizer_cnn = optim.Adam(cnn_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:18:30.143812Z","iopub.execute_input":"2025-12-01T18:18:30.144128Z","iopub.status.idle":"2025-12-01T18:18:30.155839Z","shell.execute_reply.started":"2025-12-01T18:18:30.144107Z","shell.execute_reply":"2025-12-01T18:18:30.155130Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_model(model, train_loader, optimizer, criterion, epochs=5):\n    model.train()\n    losses = []\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for data, targets in train_loader:\n            data, targets = data.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        losses.append(epoch_loss / len(train_loader))\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {losses[-1]:.4f}\")\n    return losses\n\nstart_time = time.time()\ncnn_losses = train_model(cnn_model, train_loader, optimizer_cnn, criterion)\ncnn_time = time.time() - start_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:18:30.156608Z","iopub.execute_input":"2025-12-01T18:18:30.156884Z","iopub.status.idle":"2025-12-01T18:19:04.069166Z","shell.execute_reply.started":"2025-12-01T18:18:30.156838Z","shell.execute_reply":"2025-12-01T18:19:04.068328Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 0.2358\nEpoch 2/5, Loss: 0.0883\nEpoch 3/5, Loss: 0.0643\nEpoch 4/5, Loss: 0.0546\nEpoch 5/5, Loss: 0.0453\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def evaluate_model(model, loader):\n    model.eval()\n    preds = []\n    labels = []\n    with torch.no_grad():\n        for data, targets in loader:\n            data = data.to(device)\n            outputs = model(data)\n            preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n            labels.extend(targets.numpy())\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='macro')\n    return acc, f1\n\ncnn_acc, cnn_f1 = evaluate_model(cnn_model, test_loader)\nprint(f\"CNN - Acc: {cnn_acc:.4f}, F1: {cnn_f1:.4f}, Time: {cnn_time:.2f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:19:04.070034Z","iopub.execute_input":"2025-12-01T18:19:04.070417Z","iopub.status.idle":"2025-12-01T18:19:04.878308Z","shell.execute_reply.started":"2025-12-01T18:19:04.070397Z","shell.execute_reply":"2025-12-01T18:19:04.877603Z"}},"outputs":[{"name":"stdout","text":"CNN - Acc: 0.9927, F1: 0.9927, Time: 33.91s\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class MNISTDetection(Dataset):\n    def __init__(self, images, labels):\n        self.images = images.astype(np.float32) / 255.0\n        self.labels = labels\n        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img = self.transform(self.images[idx])\n        boxes = torch.tensor([[0, 0, 28, 28]], dtype=torch.float32)\n        labels = torch.tensor([self.labels[idx] + 1], dtype=torch.int64)\n        target = {'boxes': boxes, 'labels': labels}\n        return img, target\n\ntrain_det = MNISTDetection(train_images, train_labels)\ntest_det = MNISTDetection(test_images, test_labels)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_loader_det = DataLoader(train_det, batch_size=4, shuffle=True, collate_fn=collate_fn)\ntest_loader_det = DataLoader(test_det, batch_size=4, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:19:04.880304Z","iopub.execute_input":"2025-12-01T18:19:04.880500Z","iopub.status.idle":"2025-12-01T18:19:04.997706Z","shell.execute_reply.started":"2025-12-01T18:19:04.880485Z","shell.execute_reply":"2025-12-01T18:19:04.997021Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"rcnn_model = fasterrcnn_resnet50_fpn(pretrained=True)\nin_features = rcnn_model.roi_heads.box_predictor.cls_score.in_features\nrcnn_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 11)\nrcnn_model = rcnn_model.to(device)\noptimizer_rcnn = optim.Adam(rcnn_model.parameters(), lr=0.001)\n\ndef train_rcnn(model, loader, optimizer, epochs=5):\n    model.train()\n    losses = []\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for images, targets in loader:\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            loss_dict = model(images, targets)\n            loss = sum(loss for loss in loss_dict.values())\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        losses.append(epoch_loss / len(loader))\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {losses[-1]:.4f}\")\n    return losses\n\nstart_time = time.time()\nrcnn_losses = train_rcnn(rcnn_model, train_loader_det, optimizer_rcnn)\nrcnn_time = time.time() - start_time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_rcnn(model, loader):\n    model.eval()\n    preds = []\n    labels = []\n    with torch.no_grad():\n        for images, targets in loader:\n            images = [img.to(device) for img in images]\n            outputs = model(images)\n            for out, tgt in zip(outputs, targets):\n                if len(out['labels']) > 0:\n                    pred = out['labels'][torch.argmax(out['scores'])].item() - 1\n                    preds.append(pred)\n                    labels.append(tgt['labels'][0].item() - 1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='macro')\n    return acc, f1\n\nrcnn_acc, rcnn_f1 = evaluate_rcnn(rcnn_model, test_loader_det)\nprint(f\"Faster R-CNN - Acc: {rcnn_acc:.4f}, F1: {rcnn_f1:.4f}, Time: {rcnn_time:.2f}s\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Comparaison CNN vs Faster R-CNN:\")\nprint(f\"CNN: Acc {cnn_acc:.4f}, F1 {cnn_f1:.4f}, Time {cnn_time:.2f}s\")\nprint(f\"Faster R-CNN: Acc {rcnn_acc:.4f}, F1 {rcnn_f1:.4f}, Time {rcnn_time:.2f}s\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vgg_model = vgg16(pretrained=True)\nvgg_model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\nvgg_model.classifier[6] = nn.Linear(vgg_model.classifier[6].in_features, 10)\nvgg_model = vgg_model.to(device)\noptimizer_vgg = optim.Adam(vgg_model.parameters(), lr=0.001)\n\nstart_time = time.time()\nvgg_losses = train_model(vgg_model, train_loader, optimizer_vgg, criterion)\nvgg_time = time.time() - start_time\n\nvgg_acc, vgg_f1 = evaluate_model(vgg_model, test_loader)\nprint(f\"VGG16 - Acc: {vgg_acc:.4f}, F1: {vgg_f1:.4f}, Time: {vgg_time:.2f}s\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"alex_model = alexnet(pretrained=True)\nalex_model.features[0] = nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2)\nalex_model.classifier[6] = nn.Linear(alex_model.classifier[6].in_features, 10)\nalex_model = alex_model.to(device)\noptimizer_alex = optim.Adam(alex_model.parameters(), lr=0.001)\n\nstart_time = time.time()\nalex_losses = train_model(alex_model, train_loader, optimizer_alex, criterion)\nalex_time = time.time() - start_time\n\nalex_acc, alex_f1 = evaluate_model(alex_model, test_loader)\nprint(f\"AlexNet - Acc: {alex_acc:.4f}, F1: {alex_f1:.4f}, Time: {alex_time:.2f}s\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Comparaison Tous Modèles Part 1:\")\nprint(f\"CNN: Acc {cnn_acc:.4f}, F1 {cnn_f1:.4f}, Time {cnn_time:.2f}s\")\nprint(f\"Faster R-CNN: Acc {rcnn_acc:.4f}, F1 {rcnn_f1:.4f}, Time {rcnn_time:.2f}s\")\nprint(f\"VGG16: Acc {vgg_acc:.4f}, F1 {vgg_f1:.4f}, Time {vgg_time:.2f}s\")\nprint(f\"AlexNet: Acc {alex_acc:.4f}, F1 {alex_f1:.4f}, Time {alex_time:.2f}s\")\n\n# Conclusion\nprint(\"Conclusion: Fine-tuned VGG16/AlexNet surpassent CNN ; Faster R-CNN overkill pour classification.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def patchify(images, n_patches):\n    n, c, h, w = images.shape\n    patch_size = h // n_patches\n    patches = torch.zeros(n, n_patches ** 2, c * patch_size ** 2, device=images.device)\n    for idx, image in enumerate(images):\n        for i in range(n_patches):\n            for j in range(n_patches):\n                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n                patches[idx, i * n_patches + j] = patch.flatten()\n    return patches\n\ndef get_positional_embeddings(sequence_length, d):\n    result = torch.ones(sequence_length, d)\n    for i in range(sequence_length):\n        for j in range(d):\n            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n    return result\n\nclass MyViT(nn.Module):\n    def __init__(self, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n        super().__init__()\n        self.n_patches = n_patches\n        self.hidden_d = hidden_d\n        self.linear_mapper = nn.Linear(1 * (28 // n_patches) ** 2, hidden_d)\n        self.class_token = nn.Parameter(torch.rand(1, hidden_d))\n        self.pos_embed = nn.Parameter(get_positional_embeddings(n_patches ** 2 + 1, hidden_d))\n        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n        self.mlp = nn.Linear(hidden_d, out_d)\n\nclass MyViTBlock(nn.Module):\n    def __init__(self, hidden_d, n_heads):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(hidden_d)\n        self.attn = nn.MultiheadAttention(hidden_d, n_heads)\n        self.norm2 = nn.LayerNorm(hidden_d)\n        self.mlp = nn.Sequential(nn.Linear(hidden_d, hidden_d * 4), nn.GELU(), nn.Linear(hidden_d * 4, hidden_d))\n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nvit_model = MyViT().to(device)\noptimizer_vit = optim.Adam(vit_model.parameters(), lr=0.001)\n\ndef forward_vit(model, images):\n    patches = patchify(images, model.n_patches)\n    tokens = model.linear_mapper(patches)\n    tokens = torch.stack([torch.vstack((model.class_token, token)) for token in tokens])\n    pos_embed = model.pos_embed.repeat(images.shape[0], 1, 1).to(device)\n    out = tokens + pos_embed\n    for block in model.blocks:\n        out = block(out)\n    out = out[:, 0]\n    return model.mlp(out)\n\nstart_time = time.time()\nvit_losses = train_model(vit_model, train_loader, optimizer_vit, criterion)\nvit_time = time.time() - start_time\n\nvit_acc, vit_f1 = evaluate_model(vit_model, test_loader)\nprint(f\"ViT - Acc: {vit_acc:.4f}, F1: {vit_f1:.4f}, Time: {vit_time:.2f}s\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Comparaison ViT vs Part 1:\")\nprint(f\"ViT: Acc {vit_acc:.4f}, F1 {vit_f1:.4f}, Time {vit_time:.2f}s\")\n# Ajoutez autres\n\n# Interprétation\nprint(\"ViT capture global features via attention, compétitif avec CNN mais plus compute-intensive sur MNIST.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}